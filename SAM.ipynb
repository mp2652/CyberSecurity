{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSZgDmNmu5Bv"
      },
      "source": [
        "PREPROCESS.PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgTFfNABshk0",
        "outputId": "c47943ef-c2c2-47fd-b8dc-9cca1d34663f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dpkt in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.9.8)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install dpkt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cmoncDOasMtM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import dpkt\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uHLwQ2hTs7p7"
      },
      "outputs": [],
      "source": [
        "protocols = ['dns', 'smtp', 'ssh', 'ftp', 'http', 'https']\n",
        "ports = [53, 25, 22, 21, 80, 443]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4StObm_NV5R"
      },
      "source": [
        "\n",
        "1. Importation des modules :\n",
        "   - `import numpy as np` : Importe la bibliothèque NumPy sous l'alias `np`.\n",
        "   - `import dpkt` : Importe le module `dpkt`, utilisé pour travailler avec des paquets de données réseau.\n",
        "   - `import random` : Importe le module `random`, utilisé pour générer des nombres aléatoires.\n",
        "   - `import pickle` : Importe le module `pickle`, utilisé pour la sérialisation et la désérialisation d'objets Python.\n",
        "\n",
        "2. Définition des variables :\n",
        "   - `protocols` : Une liste contenant les noms des protocoles réseau.\n",
        "   - `ports` : Une liste contenant les numéros de port associés à chaque protocole.\n",
        "\n",
        "3. Définition de la fonction `gen_flows(pcap)` :\n",
        "   - Cette fonction prend un objet `pcap` comme argument, qui est supposé être un fichier de capture réseau au format pcap.\n",
        "   - Elle initialise une liste `flows` qui contiendra les flux de données pour chaque protocole.\n",
        "   - Elle parcourt ensuite chaque paquet dans le fichier pcap et vérifie le protocole et le port associés à chaque paquet.\n",
        "   - Si un paquet correspond à un protocole et à un port spécifiés, il est ajouté au flux de données correspondant.\n",
        "   - La fonction retourne une liste de flux de données, où chaque élément de la liste correspond à un protocole réseau, et chaque flux de données est un dictionnaire où les clés sont les adresses IP source et destination, ainsi que les ports, et les valeurs sont des listes de paquets IP correspondant à ce flux.\n",
        "\n",
        "4. Dans le code principal (non inclus dans l'extrait que vous avez fourni), la fonction `gen_flows()` est probablement appelée pour traiter un fichier pcap et générer des flux de données en fonction des protocoles et des ports spécifiés.\n",
        "\n",
        "Un objet `pcap` est une structure de données qui représente un fichier de capture réseau au format pcap. Pcap est un format de fichier standard utilisé pour enregistrer et stocker des données de trafic réseau capturées à partir d'une interface réseau ou d'un fichier de journalisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xY1US1ZGs1Tz"
      },
      "outputs": [],
      "source": [
        "def gen_flows(pcap):\n",
        "\tflows = [{} for _ in range(len(protocols))]\n",
        "\n",
        "\tif pcap.datalink() != dpkt.pcap.DLT_EN10MB:\n",
        "\t\tprint('unknow data link!')\n",
        "\t\treturn\n",
        "\n",
        "\txgr = 0\n",
        "\tfor _, buff in pcap:\n",
        "\t\teth = dpkt.ethernet.Ethernet(buff)\n",
        "\t\txgr += 1\n",
        "\t\tif xgr % 500000 == 0:\n",
        "\t\t\tprint('The %dth pkt!'%xgr)\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tif isinstance(eth.data, dpkt.ip.IP) and (\n",
        "\t\tisinstance(eth.data.data, dpkt.udp.UDP)\n",
        "\t\tor isinstance(eth.data.data, dpkt.tcp.TCP)):\n",
        "\t\t\t# tcp or udp packet\n",
        "\t\t\tip = eth.data\n",
        "\n",
        "\t\t\t# loop all protocols\n",
        "\t\t\tfor name in protocols:\n",
        "\t\t\t\tindex = protocols.index(name)\n",
        "\t\t\t\tif ip.data.sport == ports[index] or \\\n",
        "\t\t\t\tip.data.dport == ports[index]:\n",
        "\t\t\t\t\tif len(flows[index]) >= 10000:\n",
        "\t\t\t\t\t\t# each class has at most 1w flows\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\t# match a protocol\n",
        "\t\t\t\t\tkey = '.'.join(map(str, map(int, ip.src))) + \\\n",
        "\t\t\t\t\t'.' + '.'.join(map(str, map(int, ip.dst))) + \\\n",
        "\t\t\t\t\t'.' + '.'.join(map(str, [ip.p, ip.data.sport, ip.data.dport]))\n",
        "\n",
        "\t\t\t\t\tif key not in flows[index]:\n",
        "\t\t\t\t\t\tflows[index][key] = [ip]\n",
        "\t\t\t\t\telif len(flows[index][key]) < 1000:\n",
        "\t\t\t\t\t\t# each flow has at most 1k flows\n",
        "\t\t\t\t\t\tflows[index][key].append(ip)\n",
        "\t\t\t\t\t# after match a protocol quit\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\treturn flows\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OAp-GNautRG9"
      },
      "outputs": [],
      "source": [
        "def closure(flows):\n",
        "\tflow_dict = {}\n",
        "\tfor name in protocols:\n",
        "\t\tindex = protocols.index(name)\n",
        "\t\tflow_dict[name] = flows[index]\n",
        "\t\tprint('============================')\n",
        "\t\tprint('Generate flows for %s'%name)\n",
        "\t\tprint('Total flows: ', len(flows[index]))\n",
        "\t\tcnt = 0\n",
        "\t\tfor k, v in flows[index].items():\n",
        "\t\t\tcnt += len(v)\n",
        "\t\tprint('Total pkts: ', cnt)\n",
        "\n",
        "\twith open('pro_flows.pkl', 'wb') as f:\n",
        "\t\tpickle.dump(flow_dict, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "DGdlZQFhtYDY",
        "outputId": "1d52393f-c3b7-4fdf-c941-839418e59459"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'from google.colab import drive\\ndrive.mount(\"/content/gdrive\")\\npcap = dpkt.pcap.Reader(open(\\'gdrive/MyDrive/202006101400.pcap\\', \\'rb\\'))\\nflows = gen_flows(pcap)\\nclosure(flows)'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\"\"\"from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "pcap = dpkt.pcap.Reader(open('gdrive/MyDrive/202006101400.pcap', 'rb'))\n",
        "flows = gen_flows(pcap)\n",
        "closure(flows)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-ln9Y_7vA5Z"
      },
      "source": [
        "TOOL.PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.66.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\abayev\\appdata\\roaming\\python\\python312\\site-packages (from tqdm) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QZ6RzRrBtfvv"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import dpkt\n",
        "import random\n",
        "import numpy as np\n",
        "#from preprocess import protocols\n",
        "from tqdm import tqdm, trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GJw89tjxtvIR"
      },
      "outputs": [],
      "source": [
        "ip_features = {'hl':1,'tos':1,'len':2,'df':1,'mf':1,'ttl':1,'p':1}\n",
        "tcp_features = {'off':1,'flags':1,'win':2}\n",
        "udp_features = {'ulen':2}\n",
        "max_byte_len = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "arcMKRbXuGQt"
      },
      "outputs": [],
      "source": [
        "def mask(p):\n",
        "\tp.src = b'\\x00\\x00\\x00\\x00'\n",
        "\tp.dst = b'\\x00\\x00\\x00\\x00'\n",
        "\tp.sum = 0\n",
        "\tp.id = 0\n",
        "\tp.offset = 0\n",
        "\n",
        "\tif isinstance(p.data, dpkt.tcp.TCP):\n",
        "\t\tp.data.sport = 0\n",
        "\t\tp.data.dport = 0\n",
        "\t\tp.data.seq = 0\n",
        "\t\tp.data.ack = 0\n",
        "\t\tp.data.sum = 0\n",
        "\n",
        "\telif isinstance(p.data, dpkt.udp.UDP):\n",
        "\t\tp.data.sport = 0\n",
        "\t\tp.data.dport = 0\n",
        "\t\tp.data.sum = 0\n",
        "\n",
        "\treturn p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OsOQCrHzuKXx"
      },
      "outputs": [],
      "source": [
        "def pkt2feature(data, k):\n",
        "\tflow_dict = {'train':{}, 'test':{}}\n",
        "\n",
        "\t# train->protocol->flowid->[pkts]\n",
        "\tfor p in protocols:\n",
        "\t\tflow_dict['train'][p] = []\n",
        "\t\tflow_dict['test'][p] = []\n",
        "\t\tall_pkts = []\n",
        "\t\tp_keys = list(data[p].keys())\n",
        "\n",
        "\t\tfor flow in p_keys:\n",
        "\t\t\tpkts = data[p][flow]\n",
        "\t\t\tall_pkts.extend(pkts)\n",
        "\t\trandom.Random(1024).shuffle(all_pkts)\n",
        "\n",
        "\t\tfor idx in range(len(all_pkts)):\n",
        "\t\t\tpkt = mask(all_pkts[idx])\n",
        "\t\t\traw_byte = pkt.pack()\n",
        "\n",
        "\t\t\tbyte = []\n",
        "\t\t\tpos = []\n",
        "\t\t\tfor x in range(min(len(raw_byte),max_byte_len)):\n",
        "\t\t\t\tbyte.append(int(raw_byte[x]))\n",
        "\t\t\t\tpos.append(x)\n",
        "\n",
        "\t\t\tbyte.extend([0]*(max_byte_len-len(byte)))\n",
        "\t\t\tpos.extend([0]*(max_byte_len-len(pos)))\n",
        "\t\t\t# if len(byte) != max_byte_len or len(pos) != max_byte_len:\n",
        "\t\t\t# \tprint(len(byte), len(pos))\n",
        "\t\t\t# \tinput()\n",
        "\t\t\tif idx in range(k*int(len(all_pkts)*0.1), (k+1)*int(len(all_pkts)*0.1)):\n",
        "\t\t\t\tflow_dict['test'][p].append((byte, pos))\n",
        "\t\t\telse:\n",
        "\t\t\t\tflow_dict['train'][p].append((byte, pos))\n",
        "\treturn flow_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hHPUGziwuS-v"
      },
      "outputs": [],
      "source": [
        "def load_epoch_data(flow_dict, train='train'):\n",
        "\tflow_dict = flow_dict[train]\n",
        "\tx, y, label = [], [], []\n",
        "\n",
        "\tfor p in protocols:\n",
        "\t\tpkts = flow_dict[p]\n",
        "\t\tfor byte, pos in pkts:\n",
        "\t\t\tx.append(byte)\n",
        "\t\t\ty.append(pos)\n",
        "\t\t\tlabel.append(protocols.index(p))\n",
        "\n",
        "\treturn np.array(x), np.array(y), np.array(label)[:, np.newaxis]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6PcFMq_OuWrj"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpro_flows.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 2\u001b[0m \tdata \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m10\u001b[39m, mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \\\n\u001b[0;32m      5\u001b[0m \tdesc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  - (Building fold dataset)   \u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m \tflow_dict \u001b[38;5;241m=\u001b[39m pkt2feature(data, i)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\t'''with open('pro_flows.pkl','rb') as f:\n",
        "\t\tdata = pickle.load(f)\n",
        "\n",
        "\tfor i in trange(10, mininterval=2, \\\n",
        "\t\tdesc='  - (Building fold dataset)   ', leave=False):\n",
        "\t\tflow_dict = pkt2feature(data, i)\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'wb') as f:\n",
        "\t\t\tpickle.dump(flow_dict, f)'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBAiVtaFvHGB"
      },
      "source": [
        "SAM.PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wqQ21zoNufOh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ArMcz_W0uyg_"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(2020)\n",
        "torch.cuda.manual_seed_all(2020)\n",
        "np.random.seed(2020)\n",
        "random.seed(2020)\n",
        "torch.backends.cudnn.deterministic = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nCeuiAOwvMZG"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "\t\"\"\"docstring for SelfAttention\"\"\"\n",
        "\tdef __init__(self, d_dim=256, dropout=0.1):\n",
        "\t\tsuper(SelfAttention, self).__init__()\n",
        "\t\t# for query, key, value, output\n",
        "\t\tself.dim = d_dim\n",
        "\t\tself.linears = nn.ModuleList([nn.Linear(d_dim, d_dim) for _ in range(4)])\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef attention(self, query, key, value):\n",
        "\t\tscores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dim)\n",
        "\t\tscores = F.softmax(scores, dim=-1)\n",
        "\t\treturn scores\n",
        "\n",
        "\tdef forward(self, query, key, value):\n",
        "\t\t# 1) query, key, value\n",
        "\t\tquery, key, value = \\\n",
        "\t\t[l(x) for l, x in zip(self.linears, (query, key, value))]\n",
        "\n",
        "\t\t# 2) Apply attention\n",
        "\t\tscores = self.attention(query, key, value)\n",
        "\t\tx = torch.matmul(scores, value)\n",
        "\n",
        "\t\t# 3) apply the final linear\n",
        "\t\tx = self.linears[-1](x.contiguous())\n",
        "\t\t# sum keepdim=False\n",
        "\t\treturn self.dropout(x), torch.mean(scores, dim=-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "AH6wTMykvRAw"
      },
      "outputs": [],
      "source": [
        "class OneDimCNN(nn.Module):\n",
        "\t\"\"\"docstring for OneDimCNN\"\"\"\n",
        "\t# https://blog.csdn.net/sunny_xsc1994/article/details/82969867\n",
        "\tdef __init__(self, max_byte_len, d_dim=256, \\\n",
        "\t\tkernel_size = [3, 4], filters=256, dropout=0.1):\n",
        "\t\tsuper(OneDimCNN, self).__init__()\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.convs = nn.ModuleList([\n",
        "\t\t\t\t\t\tnn.Sequential(nn.Conv1d(in_channels=d_dim,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tout_channels=filters,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tkernel_size=h),\n",
        "\t\t\t\t\t\t#nn.BatchNorm1d(num_features=config.feature_size),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t# MaxPool1d:\n",
        "\t\t\t\t\t\t# stride – the stride of the window. Default value is kernel_size\n",
        "\t\t\t\t\t\tnn.MaxPool1d(kernel_size=max_byte_len-h+1))\n",
        "\t\t\t\t\t\tfor h in self.kernel_size\n",
        "\t\t\t\t\t\t]\n",
        "\t\t\t\t\t\t)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = [conv(x.transpose(-2,-1)) for conv in self.convs]\n",
        "\t\tout = torch.cat(out, dim=1)\n",
        "\t\tout = out.view(-1, out.size(1))\n",
        "\t\treturn self.dropout(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "feuAzvoVvUjt"
      },
      "outputs": [],
      "source": [
        "class SAM(nn.Module):\n",
        "\t\"\"\"docstring for SAM\"\"\"\n",
        "\t# total header bytes 24\n",
        "\tdef __init__(self, num_class, max_byte_len, kernel_size = [3, 4], \\\n",
        "\t\td_dim=256, dropout=0.1, filters=256):\n",
        "\t\tsuper(SAM, self).__init__()\n",
        "\t\tself.posembedding = nn.Embedding(num_embeddings=max_byte_len,\n",
        "\t\t\t\t\t\t\t\tembedding_dim=d_dim)\n",
        "\t\tself.byteembedding = nn.Embedding(num_embeddings=300,\n",
        "\t\t\t\t\t\t\t\tembedding_dim=d_dim)\n",
        "\t\tself.attention = SelfAttention(d_dim, dropout)\n",
        "\t\tself.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "\t\tself.fc = nn.Linear(in_features=256*len(kernel_size),\n",
        "                            out_features=num_class)\n",
        "\n",
        "\tdef forward(self, x, y):\n",
        "\t\tout = self.byteembedding(x) + self.posembedding(y)\n",
        "\t\tout, score = self.attention(out, out, out)\n",
        "\t\tout = self.cnn(out)\n",
        "\t\tout = self.fc(out)\n",
        "\t\tif not self.training:\n",
        "\t\t\treturn F.softmax(out, dim=-1).max(1)[1], score\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "nfLY_-nevYeZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.2203, -0.0799, -0.0033, -0.0247, -0.1041],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(2) tensor([0.0409, 0.0561, 0.0431, 0.0441, 0.0604, 0.0449, 0.0791, 0.0465, 0.0471,\n",
            "        0.0453, 0.0514, 0.0312, 0.0553, 0.0426, 0.0486, 0.0581, 0.0389, 0.0523,\n",
            "        0.0626, 0.0514], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "\tx = np.random.randint(0, 255, (10, 20))\n",
        "\ty = np.random.randint(0, 20, (10, 20))\n",
        "\tsam = SAM(num_class=5, max_byte_len=20)\n",
        "\tout = sam(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "\tprint(out[0])\n",
        "\n",
        "\tsam.eval()\n",
        "\tout, score = sam(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "\tprint(out[0], score[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFWhN2zrviK4"
      },
      "source": [
        "TRAIN.PY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.4.1.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pFQbRVyVvcif"
      },
      "outputs": [],
      "source": [
        "#import sklearn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import argparse\n",
        "import time\n",
        "from tqdm import tqdm, trange\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import pickle\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4Nbnzu45vkIG"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "\t\"\"\"docstring for Dataset\"\"\"\n",
        "\tdef __init__(self, x, y, label):\n",
        "\t\tsuper(Dataset, self).__init__()\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\t\tself.label = label\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.x)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn self.x[idx], self.y[idx], self.label[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "0KEbwPuqvp0O"
      },
      "outputs": [],
      "source": [
        "def paired_collate_fn(insts):\n",
        "\tx, y, label = list(zip(*insts))\n",
        "\treturn torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "mTZYB8iVvtUq"
      },
      "outputs": [],
      "source": [
        "def cal_loss(pred, gold, cls_ratio=None):\n",
        "\tgold = gold.contiguous().view(-1)\n",
        "\t# By default, the losses are averaged over each loss element in the batch.\n",
        "\tloss = F.cross_entropy(pred, gold)\n",
        "\n",
        "\t# torch.max(a,0) 返回每一列中最大值的那个元素，且返回索引\n",
        "\tpred = F.softmax(pred, dim = -1).max(1)[1]\n",
        "\t# 相等位置输出1，否则0\n",
        "\tn_correct = pred.eq(gold)\n",
        "\tacc = n_correct.sum().item() / n_correct.shape[0]\n",
        "\n",
        "\treturn loss, acc*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "grHi5Hm8vxzY"
      },
      "outputs": [],
      "source": [
        "def test_epoch(model, test_data):\n",
        "\t''' Epoch operation in training phase'''\n",
        "\tmodel.eval()\n",
        "\n",
        "\ttotal_acc = []\n",
        "\ttotal_pred = []\n",
        "\ttotal_score = []\n",
        "\ttotal_time = []\n",
        "\t# tqdm: 进度条库\n",
        "\t# desc ：进度条的描述\n",
        "\t# leave：把进度条的最终形态保留下来 bool\n",
        "\t# mininterval：最小进度更新间隔，以秒为单位\n",
        "\tfor batch in tqdm(\n",
        "\t\ttest_data, mininterval=2,\n",
        "\t\tdesc='  - (Testing)   ', leave=False):\n",
        "\n",
        "\t\t# prepare data\n",
        "\t\tsrc_seq, src_seq2, gold = batch\n",
        "\t\tsrc_seq, src_seq2, gold = src_seq.cuda(), src_seq2.cuda(), gold.cuda()\n",
        "\t\tgold = gold.contiguous().view(-1)\n",
        "\n",
        "\t\t# forward\n",
        "\t\ttorch.cuda.synchronize()\n",
        "\t\tstart = time.time()\n",
        "\t\tpred, score = model(src_seq, src_seq2)\n",
        "\t\ttorch.cuda.synchronize()\n",
        "\t\tend = time.time()\n",
        "\t\t# 相等位置输出1，否则0\n",
        "\t\tn_correct = pred.eq(gold)\n",
        "\t\tacc = n_correct.sum().item()*100 / n_correct.shape[0]\n",
        "\t\ttotal_acc.append(acc)\n",
        "\t\ttotal_pred.extend(pred.long().tolist())\n",
        "\t\ttotal_score.append(torch.mean(score, dim=0).tolist())\n",
        "\t\ttotal_time.append(end - start)\n",
        "\n",
        "\treturn sum(total_acc)/len(total_acc), np.array(total_score).mean(axis=0), \\\n",
        "\ttotal_pred, sum(total_time)/len(total_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HbMb2fTmwOP"
      },
      "source": [
        "Without cuda and Torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "4FWXUfmOmm0O"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def test_epoch1(model, test_data):\n",
        "    ''' Epoch operation in training phase'''\n",
        "    model.eval()\n",
        "\n",
        "    total_acc = []\n",
        "    total_pred = []\n",
        "    total_score = []\n",
        "    total_time = []\n",
        "\n",
        "    for batch in tqdm(test_data, mininterval=2, desc='  - (Testing)   ', leave=False):\n",
        "        # prepare data\n",
        "        src_seq, src_seq2, gold = batch\n",
        "        # No need for CUDA operations here\n",
        "        #src_seq, src_seq2, gold = src_seq.cuda(), src_seq2.cuda(), gold.cuda()\n",
        "        src_seq, src_seq2, gold = src_seq, src_seq2, gold\n",
        "        gold = gold.contiguous().view(-1)\n",
        "\n",
        "        # forward\n",
        "        start = time.time()\n",
        "        pred, score = model(src_seq, src_seq2)\n",
        "        end = time.time()\n",
        "\n",
        "        # Calculate accuracy\n",
        "        n_correct = pred.eq(gold)\n",
        "        acc = n_correct.sum().item() * 100 / n_correct.shape[0]\n",
        "        total_acc.append(acc)\n",
        "\n",
        "        # Append predictions, scores, and time\n",
        "        total_pred.extend(pred.long().tolist())\n",
        "        total_score.append(np.mean(score.detach().cpu().numpy(), axis=0))\n",
        "        total_time.append(end - start)\n",
        "\n",
        "    # Calculate average accuracy, score, and time\n",
        "    avg_acc = sum(total_acc) / len(total_acc)\n",
        "    avg_score = np.mean(total_score, axis=0)\n",
        "    avg_time = sum(total_time) / len(total_time)\n",
        "\n",
        "    return avg_acc, avg_score, total_pred, avg_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zJnRvwWuv6Gl"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, training_data, optimizer):\n",
        "\t''' Epoch operation in training phase'''\n",
        "\tmodel.train()\n",
        "\n",
        "\ttotal_loss = []\n",
        "\ttotal_acc = []\n",
        "\t# tqdm: 进度条库\n",
        "\t# desc ：进度条的描述\n",
        "\t# leave：把进度条的最终形态保留下来 bool\n",
        "\t# mininterval：最小进度更新间隔，以秒为单位\n",
        "\tfor batch in tqdm(\n",
        "\t\ttraining_data, mininterval=2,\n",
        "\t\tdesc='  - (Training)   ', leave=False):\n",
        "\n",
        "\t\t# prepare data\n",
        "\t\tsrc_seq, src_seq2, gold = batch\n",
        "\t\tsrc_seq, src_seq2, gold = src_seq.cuda(), src_seq2.cuda(), gold.cuda()\n",
        "\t\t#src_seq, src_seq2, gold = src_seq, src_seq2, gold\n",
        "\t\toptimizer.zero_grad()\n",
        "\t\t# forward\n",
        "\t\tpred = model(src_seq, src_seq2)\n",
        "\t\tloss_per_batch, acc_per_batch = cal_loss(pred, gold)\n",
        "\t\t# update parameters\n",
        "\t\tloss_per_batch.backward()\n",
        "\t\toptimizer.step()\n",
        "\n",
        "\t\t# 只有一个元素，可以用item取而不管维度\n",
        "\t\ttotal_loss.append(loss_per_batch.item())\n",
        "\t\ttotal_acc.append(acc_per_batch)\n",
        "\n",
        "\treturn sum(total_loss)/len(total_loss), sum(total_acc)/len(total_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Hy14El1MwDV7"
      },
      "outputs": [],
      "source": [
        "def main(i, flow_dict):\n",
        "\tf = open('results_%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\tmodel = SAM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(3, mininterval=2, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\ttest_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "\t\twith open('atten_%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('metric_%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('cm_%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\n",
        "\tf.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#pip uninstall torch torchvision torchaudio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "print(torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.1)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cu124/torch_stable.html\n",
            "Requirement already satisfied: torch in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
            "Requirement already satisfied: torchvision in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\abayev\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#pip install torch torchvision -f https://download.pytorch.org/whl/cu124/torch_stable.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0.dev20240306+cu121\n",
            "12.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDkBKoPfwLla",
        "outputId": "eb81daaf-c087-4f4f-acac-159a6ae8e438"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :   0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\abayev\\AppData\\Local\\Temp\\ipykernel_16056\\2161614021.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
            "  return torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(label)\n",
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "\n",
        "\tfor i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmain(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--RESIDUAL LAYER--"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SAMRES(nn.Module):\n",
        "    \"\"\"docstring for ImprovedSAM\"\"\"\n",
        "    # total header bytes 24\n",
        "    def __init__(self, num_class, max_byte_len, kernel_size=[3, 4], \\\n",
        "        d_dim=256, dropout=0.1, filters=256):\n",
        "        super(SAMRES, self).__init__()\n",
        "        self.posembedding = nn.Embedding(num_embeddings=max_byte_len,\n",
        "                                          embedding_dim=d_dim)\n",
        "        self.byteembedding = nn.Embedding(num_embeddings=300,\n",
        "                                          embedding_dim=d_dim)\n",
        "        self.attention = SelfAttention(d_dim, dropout)\n",
        "        self.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "        self.fc = nn.Linear(in_features=filters * len(kernel_size),\n",
        "                            out_features=num_class)\n",
        "\n",
        "        # Residual connection\n",
        "        self.residual_conv = nn.Sequential(\n",
        "            nn.Conv1d(in_channels=d_dim, out_channels=filters, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.byteembedding(x) + self.posembedding(y)\n",
        "        out, score = self.attention(out, out, out)\n",
        "        out = self.residual_conv(out.transpose(1, 2)).transpose(1, 2) + out  # Residual connection\n",
        "        out = self.cnn(out)\n",
        "        out = self.fc(out)\n",
        "        if not self.training:\n",
        "            return F.softmax(out, dim=-1).max(1)[1], score\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0545,  0.0886,  0.1150,  0.0472,  0.0651],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(4) tensor([0.0406, 0.0588, 0.0706, 0.0430, 0.0497, 0.0797, 0.0379, 0.0353, 0.0473,\n",
            "        0.0526, 0.0559, 0.0450, 0.0485, 0.0442, 0.0515, 0.0631, 0.0381, 0.0417,\n",
            "        0.0480, 0.0485], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randint(0, 255, (10, 20))\n",
        "y = np.random.randint(0, 20, (10, 20))\n",
        "samres = SAMRES(num_class=5, max_byte_len=20)\n",
        "out = samres(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0])\n",
        "\n",
        "samres.eval()\n",
        "out, score = samres(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0], score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainres(i, flow_dict):\n",
        "\tf = open('results_res_%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\tmodel = SAMRES(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(3, mininterval=100, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\ttest_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "\t\twith open('atten_res_%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('metric_res_%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tprint(\"a:\",a,\"b:\",b,\"c:\",c)\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('cm_res_%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\t\t# # early stop\n",
        "\t\t# if len(loss_list) == 5:\n",
        "\t\t# \tif abs(sum(loss_list)/len(loss_list) - train_loss) < 0.005:\n",
        "\t\t# \t\tbreak\n",
        "\t\t# \tloss_list[epoch_i%len(loss_list)] = train_loss\n",
        "\t\t# else:\n",
        "\t\t# \tloss_list.append(train_loss)\n",
        "\n",
        "\tf.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :  33%|███▎      | 1/3 [03:30<07:01, 210.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a: 0.9713530189510797 b: 0.9914529914529915 c: 0.9520518358531318\n",
            "a: 0.9741759726224783 b: 0.9736498649864986 c: 0.9747026491259686\n",
            "a: 0.9709942917828223 b: 0.9734171740359983 c: 0.968583440924289\n",
            "a: 0.9054928796005178 b: 0.8568428421421072 c: 0.96\n",
            "a: 0.9327734671106985 b: 0.9192056537577009 c: 0.9467478128565995\n",
            "a: 0.861522198731501 b: 0.8895187128120269 c: 0.8352342226189295\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :  67%|██████▋   | 2/3 [07:06<03:33, 213.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a: 0.9666298342541436 b: 0.9895927601809955 c: 0.9447084233261339\n",
            "a: 0.9805831826401447 b: 0.9839638904010162 c: 0.9772256262389619\n",
            "a: 0.9731234149072824 b: 0.9682123545797149 c: 0.9780845499387559\n",
            "a: 0.9221235281966536 b: 0.9742470536883457 c: 0.8752941176470588\n",
            "a: 0.936992896340437 b: 0.9128973004887329 c: 0.962394965247761\n",
            "a: 0.8819167735005717 b: 0.9264732646598478 c: 0.8414493041092268\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a: 0.9641983544585279 b: 0.9935838680109991 c: 0.9365010799136069\n",
            "a: 0.9785062521122001 b: 0.9786605975934022 c: 0.9783519553072626\n",
            "a: 0.9725495402794835 b: 0.9751389489799315 c: 0.9699738471215281\n",
            "a: 0.94492525570417 b: 0.9479084451460142 c: 0.9419607843137255\n",
            "a: 0.9326827936811218 b: 0.9204350314825415 c: 0.9452609011376604\n",
            "a: 0.8654595771249219 b: 0.8839101061629671 c: 0.8477635624318159\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainres(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- LSTM LAYER --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAMLSTM(nn.Module):\n",
        "    \"\"\"docstring for SAM\"\"\"\n",
        "    # total header bytes 24\n",
        "    def __init__(self, num_class, max_byte_len, kernel_size=[3, 4], \\\n",
        "        d_dim=256, dropout=0.1, filters=256, lstm_hidden_size=128, lstm_num_layers=1):\n",
        "        super(SAMLSTM, self).__init__()\n",
        "        self.posembedding = nn.Embedding(num_embeddings=max_byte_len,\n",
        "                                embedding_dim=d_dim)\n",
        "        self.byteembedding = nn.Embedding(num_embeddings=300,\n",
        "                                embedding_dim=d_dim)\n",
        "        self.attention = SelfAttention(d_dim, dropout)\n",
        "        self.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "        self.fc = nn.Linear(in_features=lstm_hidden_size, out_features=num_class)\n",
        "        self.lstm = nn.LSTM(input_size=filters * len(kernel_size),\n",
        "                            hidden_size=lstm_hidden_size,\n",
        "                            num_layers=lstm_num_layers,\n",
        "                            batch_first=True)               \n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.byteembedding(x) + self.posembedding(y)\n",
        "        out, score = self.attention(out, out, out)\n",
        "        out = self.cnn(out)\n",
        "    \n",
        "    # Reshape out to a 2D tensor before passing to the linear layer\n",
        "        out, _ = self.lstm(out.unsqueeze(1))  # Unsqueeze to add a dummy sequence dimension\n",
        "        out = out[:, -1, :] \n",
        "    \n",
        "        out = self.fc(out)\n",
        "    \n",
        "        if not self.training:\n",
        "            return F.softmax(out, dim=-1).max(1)[1], score\n",
        "        return out\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.0534,  0.0236,  0.0817,  0.0177, -0.0229],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(2) tensor([0.0412, 0.0433, 0.0428, 0.0511, 0.0348, 0.0487, 0.0491, 0.0298, 0.0557,\n",
            "        0.0755, 0.0650, 0.0631, 0.0496, 0.0546, 0.0550, 0.0525, 0.0505, 0.0432,\n",
            "        0.0340, 0.0605], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randint(0, 255, (10, 20))\n",
        "y = np.random.randint(0, 20, (10, 20))\n",
        "samlstm = SAMLSTM(num_class=5, max_byte_len=20)\n",
        "out = samlstm(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0])\n",
        "\n",
        "samlstm.eval()\n",
        "out, score = samlstm(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0], score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainlstm(i, flow_dict):\n",
        "\tf = open('results_lstm_%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\tmodel = SAMLSTM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(3, mininterval=100, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\ttest_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "\t\twith open('atten_lstm_%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('metric_lstm_%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('cm_lstm_%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\t\t# # early stop\n",
        "\t\t# if len(loss_list) == 5:\n",
        "\t\t# \tif abs(sum(loss_list)/len(loss_list) - train_loss) < 0.005:\n",
        "\t\t# \t\tbreak\n",
        "\t\t# \tloss_list[epoch_i%len(loss_list)] = train_loss\n",
        "\t\t# else:\n",
        "\t\t# \tloss_list.append(train_loss)\n",
        "\n",
        "\tf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :   0%|          | 0/3 [00:00<?, ?it/s]c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "  - (Training Epochs)   :  33%|███▎      | 1/3 [03:43<07:27, 223.80s/it]c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "  - (Training Epochs)   :  67%|██████▋   | 2/3 [07:27<03:43, 223.59s/it]c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainlstm(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- BATCH NORMALIZATION --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneDimCNNBN(nn.Module):\n",
        "\t\"\"\"docstring for OneDimCNN\"\"\"\n",
        "\t# https://blog.csdn.net/sunny_xsc1994/article/details/82969867\n",
        "\tdef __init__(self, max_byte_len, d_dim=256, \\\n",
        "\t\tkernel_size = [3, 4], filters=256, dropout=0.1):\n",
        "\t\tsuper(OneDimCNNBN, self).__init__()\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.convs = nn.ModuleList([\n",
        "\t\t\t\t\t\tnn.Sequential(nn.Conv1d(in_channels=d_dim,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tout_channels=filters,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tkernel_size=h),\n",
        "\t\t\t\t\t\tnn.BatchNorm1d(num_features=filters),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t# MaxPool1d:\n",
        "\t\t\t\t\t\t# stride – the stride of the window. Default value is kernel_size\n",
        "\t\t\t\t\t\tnn.MaxPool1d(kernel_size=max_byte_len-h+1))\n",
        "\t\t\t\t\t\tfor h in self.kernel_size\n",
        "\t\t\t\t\t\t]\n",
        "\t\t\t\t\t\t)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = [conv(x.transpose(-2,-1)) for conv in self.convs]\n",
        "\t\tout = torch.cat(out, dim=1)\n",
        "\t\tout = out.view(-1, out.size(1))\n",
        "\t\treturn self.dropout(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAMBN(nn.Module):\n",
        "\t\"\"\"docstring for SAM\"\"\"\n",
        "\t# total header bytes 24\n",
        "\tdef __init__(self, num_class, max_byte_len, kernel_size = [3, 4], \\\n",
        "\t\td_dim=256, dropout=0.1, filters=256):\n",
        "\t\tsuper(SAMBN, self).__init__()\n",
        "\t\tself.posembedding = nn.Embedding(num_embeddings=max_byte_len,\n",
        "\t\t\t\t\t\t\t\tembedding_dim=d_dim)\n",
        "\t\tself.byteembedding = nn.Embedding(num_embeddings=300,\n",
        "\t\t\t\t\t\t\t\tembedding_dim=d_dim)\n",
        "\t\tself.attention = SelfAttention(d_dim, dropout)\n",
        "\t\tself.cnn = OneDimCNNBN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "\t\tself.fc = nn.Linear(in_features=256*len(kernel_size),\n",
        "                            out_features=num_class)\n",
        "\n",
        "\tdef forward(self, x, y):\n",
        "\t\tout = self.byteembedding(x) + self.posembedding(y)\n",
        "\t\tout, score = self.attention(out, out, out)\n",
        "\t\tout = self.cnn(out)\n",
        "\t\tout = self.fc(out)\n",
        "\t\tif not self.training:\n",
        "\t\t\treturn F.softmax(out, dim=-1).max(1)[1], score\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.0344,  1.1171,  0.7604,  0.4999, -1.0242],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(2) tensor([0.0385, 0.0438, 0.0543, 0.0395, 0.0533, 0.0460, 0.0581, 0.0584, 0.0376,\n",
            "        0.0608, 0.0536, 0.0510, 0.0479, 0.0447, 0.0464, 0.0587, 0.0522, 0.0483,\n",
            "        0.0643, 0.0427], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randint(0, 255, (10, 20))\n",
        "y = np.random.randint(0, 20, (10, 20))\n",
        "sambn = SAMBN(num_class=5, max_byte_len=20)\n",
        "out = sambn(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0])\n",
        "\n",
        "sambn.eval()\n",
        "out, score = sambn(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0], score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainbn(i, flow_dict):\n",
        "\tf = open('results_bn_%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\tmodel = SAMBN(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(3, mininterval=100, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\ttest_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "\t\twith open('atten_bn_%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('metric_bn_%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('cm_bn_%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\t\t# # early stop\n",
        "\t\t# if len(loss_list) == 5:\n",
        "\t\t# \tif abs(sum(loss_list)/len(loss_list) - train_loss) < 0.005:\n",
        "\t\t# \t\tbreak\n",
        "\t\t# \tloss_list[epoch_i%len(loss_list)] = train_loss\n",
        "\t\t# else:\n",
        "\t\t# \tloss_list.append(train_loss)\n",
        "\n",
        "\tf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainbn(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-- SPATIAL DROPOUT --"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneDimCNNSD(nn.Module):\n",
        "\t\"\"\"docstring for OneDimCNN\"\"\"\n",
        "\t# https://blog.csdn.net/sunny_xsc1994/article/details/82969867\n",
        "\tdef __init__(self, max_byte_len, d_dim=256, \\\n",
        "\t\tkernel_size = [3, 4], filters=256, dropout=0.1):\n",
        "\t\tsuper(OneDimCNNSD, self).__init__()\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.convs = nn.ModuleList([\n",
        "\t\t\t\t\t\tnn.Sequential(nn.Conv1d(in_channels=d_dim,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tout_channels=filters,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tkernel_size=h),\n",
        "\t\t\t\t\t\t#nn.BatchNorm1d(num_features=filters),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t# MaxPool1d:\n",
        "\t\t\t\t\t\t# stride – the stride of the window. Default value is kernel_size\n",
        "\t\t\t\t\t\tnn.MaxPool1d(kernel_size=max_byte_len-h+1),\n",
        "\t\t\t\t\t\tnn.Dropout2d(p=dropout))\n",
        "\t\t\t\t\t\tfor h in self.kernel_size\n",
        "\t\t\t\t\t\t]\n",
        "\t\t\t\t\t\t)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = [conv(x.transpose(-2,-1)) for conv in self.convs]\n",
        "\t\tout = torch.cat(out, dim=1)\n",
        "\t\tout = out.view(-1, out.size(1))\n",
        "\t\treturn self.dropout(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAMSD(nn.Module):\n",
        "\t\"\"\"docstring for SAM\"\"\"\n",
        "\t# total header bytes 24\n",
        "\tdef __init__(self, num_class, max_byte_len, kernel_size = [3, 4], \\\n",
        "\t\td_dim=256, dropout=0.1, filters=256):\n",
        "\t\tsuper(SAMSD, self).__init__()\n",
        "\t\tself.posembedding = nn.Embedding(num_embeddings=max_byte_len,\n",
        "\t\t\t\t\t\t\t\tembedding_dim=d_dim)\n",
        "\t\tself.byteembedding = nn.Embedding(num_embeddings=300,\n",
        "\t\t\t\t\t\t\t\tembedding_dim=d_dim)\n",
        "\t\tself.attention = SelfAttention(d_dim, dropout)\n",
        "\t\tself.cnn = OneDimCNNSD(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "\t\tself.fc = nn.Linear(in_features=256*len(kernel_size),\n",
        "                            out_features=num_class)\n",
        "\n",
        "\tdef forward(self, x, y):\n",
        "\t\tout = self.byteembedding(x) + self.posembedding(y)\n",
        "\t\tout, score = self.attention(out, out, out)\n",
        "\t\tout = self.cnn(out)\n",
        "\t\tout = self.fc(out)\n",
        "\t\tif not self.training:\n",
        "\t\t\treturn F.softmax(out, dim=-1).max(1)[1], score\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.0589, -0.0442,  0.0066, -0.0018,  0.0818],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor(0) tensor([0.0494, 0.0620, 0.0536, 0.0470, 0.0479, 0.0441, 0.0498, 0.0329, 0.0638,\n",
            "        0.0642, 0.0503, 0.0438, 0.0479, 0.0566, 0.0422, 0.0356, 0.0626, 0.0464,\n",
            "        0.0414, 0.0584], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ],
      "source": [
        "x = np.random.randint(0, 255, (10, 20))\n",
        "y = np.random.randint(0, 20, (10, 20))\n",
        "samsd = SAMSD(num_class=5, max_byte_len=20)\n",
        "out = samsd(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0])\n",
        "\n",
        "samsd.eval()\n",
        "out, score = samsd(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(out[0], score[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainsd(i, flow_dict):\n",
        "\tf = open('results_sd_%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\tmodel = SAMSD(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(3, mininterval=100, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\ttest_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "\t\twith open('atten_sd_%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('metric_sd_%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('cm_sd_%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\t\t# # early stop\n",
        "\t\t# if len(loss_list) == 5:\n",
        "\t\t# \tif abs(sum(loss_list)/len(loss_list) - train_loss) < 0.005:\n",
        "\t\t# \t\tbreak\n",
        "\t\t# \tloss_list[epoch_i%len(loss_list)] = train_loss\n",
        "\t\t# else:\n",
        "\t\t# \tloss_list.append(train_loss)\n",
        "\n",
        "\tf.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :   0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\abayev\\AppData\\Local\\Temp\\ipykernel_15432\\2161614021.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
            "  return torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(label)\n",
            "c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1381: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
            "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
            "  - (Training Epochs)   :  33%|███▎      | 1/3 [02:56<05:52, 176.05s/it]c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1381: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
            "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
            "  - (Training Epochs)   :  67%|██████▋   | 2/3 [05:46<02:52, 172.94s/it]c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:1381: UserWarning: dropout2d: Received a 3D input to dropout2d and assuming that channel-wise 1D dropout behavior is desired - input is interpreted as shape (N, C, L), where C is the channel dim. This behavior will change in a future release to interpret the input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D channel-wise dropout behavior, please switch to using dropout1d instead.\n",
            "  warnings.warn(\"dropout2d: Received a 3D input to dropout2d and assuming that channel-wise \"\n",
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainsd(i, flow_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "multi head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2084, -0.0758,  0.0391, -0.0643, -0.1008],\n",
            "        [-0.2907, -0.0412,  0.0855, -0.0513, -0.0134],\n",
            "        [-0.2040,  0.0444,  0.0456, -0.0121, -0.0880],\n",
            "        [-0.2368,  0.0116, -0.0561, -0.0048, -0.0496],\n",
            "        [-0.1875,  0.0198,  0.0541, -0.1247, -0.0469],\n",
            "        [-0.2301,  0.1281, -0.0541, -0.0699,  0.0156],\n",
            "        [-0.2305, -0.0129,  0.0129, -0.0475, -0.0241],\n",
            "        [-0.2025, -0.0035,  0.0553, -0.0471,  0.0039],\n",
            "        [-0.2269, -0.0351,  0.0587, -0.0241, -0.1163],\n",
            "        [-0.2030,  0.0207,  0.1049, -0.0080, -0.1280]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class OneDimCNN(nn.Module):\n",
        "    def __init__(self, max_byte_len, d_dim=256, kernel_size=[3, 4], filters=256, dropout=0.1):\n",
        "        super(OneDimCNN, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(in_channels=d_dim, out_channels=filters, kernel_size=h),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(kernel_size=max_byte_len - h + 1)\n",
        "            ) for h in self.kernel_size\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = [conv(x.transpose(-2, -1)) for conv in self.convs]\n",
        "        out = torch.cat(out, dim=1)\n",
        "        out = out.view(-1, out.size(1))\n",
        "        return self.dropout(out)\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.head_dim = d_model // nhead\n",
        "\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "\n",
        "        query, key, value = [self.split_heads(x) for x in (query, key, value)]\n",
        "\n",
        "        scores = self.attention(query, key, value)\n",
        "        x = torch.matmul(scores, value)\n",
        "\n",
        "        x = self.combine_heads(x)\n",
        "        x = self.out_linear(x)\n",
        "\n",
        "        return self.dropout(x), torch.mean(scores, dim=-2)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, features = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.nhead, self.head_dim)\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.nhead, seq_len, self.head_dim)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        x = x.view(batch_size // self.nhead, self.nhead, seq_len, -1)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size // self.nhead, seq_len, -1)\n",
        "        return x\n",
        "\n",
        "class MHSAM(nn.Module):\n",
        "    def __init__(self, num_class, max_byte_len, kernel_size=[3, 4], d_dim=256, dropout=0.1, filters=256, nhead=4):\n",
        "        super(MHSAM, self).__init__()\n",
        "        self.posembedding = nn.Embedding(num_embeddings=max_byte_len, embedding_dim=d_dim)\n",
        "        self.byteembedding = nn.Embedding(num_embeddings=300, embedding_dim=d_dim)\n",
        "        self.attention = MultiHeadAttention(d_dim, nhead, dropout)\n",
        "        self.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "        self.fc = nn.Linear(in_features=filters * len(kernel_size), out_features=num_class)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.byteembedding(x) + self.posembedding(y)\n",
        "        out, _ = self.attention(out, out, out)\n",
        "        out = self.cnn(out)\n",
        "        out = self.fc(out)\n",
        "        if not self.training:\n",
        "            return F.softmax(out, dim=-1).max(1)[1]\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "x = np.random.randint(0, 255, (10, 20))\n",
        "y = np.random.randint(0, 20, (10, 20))\n",
        "mhsam = MHSAM(num_class=5, max_byte_len=20)\n",
        "output = mhsam(torch.from_numpy(x).long(), torch.from_numpy(y).long())\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainmhsam(i, flow_dict):\n",
        "\tf = open('results_parallel%d.txt'%i, 'w')\n",
        "\tf.write('Train Loss Time Test\\n')\n",
        "\tf.flush()\n",
        "\n",
        "\tmodel = MHSAM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "\toptimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "\tloss_list = []\n",
        "\t# default epoch is 3\n",
        "\tfor epoch_i in trange(3, mininterval=100, \\\n",
        "\t\tdesc='  - (Training Epochs)   ', leave=False):\n",
        "\n",
        "\t\ttrain_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "\t\ttraining_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=train_x, y=train_y, label=train_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=True\n",
        "\t\t\t)\n",
        "\t\ttrain_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "\t\ttest_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "\t\ttest_data = torch.utils.data.DataLoader(\n",
        "\t\t\t\tDataset(x=test_x, y=test_y, label=test_label),\n",
        "\t\t\t\tnum_workers=0,\n",
        "\t\t\t\tcollate_fn=paired_collate_fn,\n",
        "\t\t\t\tbatch_size=128,\n",
        "\t\t\t\tshuffle=False\n",
        "\t\t\t)\n",
        "\t\tfor batch in tqdm(test_data, mininterval=2, desc='  - (Testing)   ', leave=False):\n",
        "   \t\t\t test_acc, score, pred, test_time = test_epoch(model, batch)\n",
        "\t\twith open('atten_mhsam%d.txt'%i, 'w') as f2:\n",
        "\t\t\tf2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "\t\t# write F1, PRECISION, RECALL\n",
        "\t\twith open('metric_mhsam%d.txt'%i, 'w') as f3:\n",
        "\t\t\tf3.write('F1 PRE REC\\n')\n",
        "\t\t\tp, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "\t\t\tfor a, b, c in zip(fscore, p, r):\n",
        "\t\t\t\t# for every cls\n",
        "\t\t\t\tf3.write('%.2f %.2f %.2f\\n'%(a, b, c))\n",
        "\t\t\t\tf3.flush()\n",
        "\t\t\tif len(fscore) != len(protocols):\n",
        "\t\t\t\ta = set(pred)\n",
        "\t\t\t\tb = set(test_label[:,0])\n",
        "\t\t\t\tf3.write('%s\\n%s'%(str(a), str(b)))\n",
        "\n",
        "\t\t# write Confusion Matrix\n",
        "\t\twith open('cm_mhsam%d.pkl'%i, 'wb') as f4:\n",
        "\t\t\tpickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "\n",
        "\t\t# write ACC\n",
        "\t\tf.write('%.2f %.4f %.6f %.2f\\n'%(train_acc, train_loss, test_time, test_acc))\n",
        "\t\tf.flush()\n",
        "\n",
        "\t\t\t# # early stop\n",
        "\t\t# if len(loss_list) == 5:\n",
        "\t\t# \tif abs(sum(loss_list)/len(loss_list) - train_loss) < 0.005:\n",
        "\t\t# \t\tbreak\n",
        "\t\t# \tloss_list[epoch_i%len(loss_list)] = train_loss\n",
        "\t\t# else:\n",
        "\t\t# \tloss_list.append(train_loss)\n",
        "\n",
        "\tf.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "\u001b[A\n",
            "                                                               \r"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \tflow_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====\u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m fold validation ====\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmainmhsam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_dict\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[27], line 32\u001b[0m, in \u001b[0;36mmainmhsam\u001b[1;34m(i, flow_dict)\u001b[0m\n\u001b[0;32m     24\u001b[0m \t\ttest_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     25\u001b[0m \t\t\t\tDataset(x\u001b[38;5;241m=\u001b[39mtest_x, y\u001b[38;5;241m=\u001b[39mtest_y, label\u001b[38;5;241m=\u001b[39mtest_label),\n\u001b[0;32m     26\u001b[0m \t\t\t\tnum_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m \t\t\t\tshuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \t\t\t)\n\u001b[0;32m     31\u001b[0m \t\t\u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(test_data, mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  - (Testing)   \u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 32\u001b[0m    \t\t\t test_acc, score, pred, test_time \u001b[38;5;241m=\u001b[39m \u001b[43mtest_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \t\t\u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matten_mhsam\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39mi, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[0;32m     34\u001b[0m \t\t\tf2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat, score)))\n",
            "Cell \u001b[1;32mIn[22], line 18\u001b[0m, in \u001b[0;36mtest_epoch\u001b[1;34m(model, test_data)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# tqdm: 进度条库\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# desc ：进度条的描述\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# leave：把进度条的最终形态保留下来 bool\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# mininterval：最小进度更新间隔，以秒为单位\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(\n\u001b[0;32m     14\u001b[0m \ttest_data, mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     15\u001b[0m \tdesc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  - (Testing)   \u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \t\u001b[38;5;66;03m# prepare data\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \tsrc_seq, src_seq2, gold \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     19\u001b[0m \tsrc_seq, src_seq2, gold \u001b[38;5;241m=\u001b[39m src_seq\u001b[38;5;241m.\u001b[39mcuda(), src_seq2\u001b[38;5;241m.\u001b[39mcuda(), gold\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     20\u001b[0m \tgold \u001b[38;5;241m=\u001b[39m gold\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 3)"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainmhsam(i, flow_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 1.2640e-02,  1.1647e-01, -1.3449e-01,  1.8673e-02, -2.8180e-02],\n",
            "        [ 7.5116e-02,  4.2536e-02, -1.8551e-01,  7.0953e-03, -4.9415e-02],\n",
            "        [ 9.9890e-02, -2.3339e-02, -9.2603e-02,  2.4429e-02,  1.0407e-01],\n",
            "        [ 3.3617e-02,  4.9561e-02, -9.4847e-02,  2.8700e-03,  6.8238e-02],\n",
            "        [ 5.4745e-02, -4.2953e-03, -1.1213e-01, -1.5701e-02,  9.8361e-04],\n",
            "        [ 1.8611e-02,  1.1650e-04, -1.5371e-01,  5.9058e-02, -3.5684e-02],\n",
            "        [-4.3111e-02, -2.0026e-02, -1.4689e-01,  3.2048e-02,  6.0493e-02],\n",
            "        [ 7.1340e-02,  8.3140e-02, -1.8492e-01,  7.7579e-02, -3.5308e-02],\n",
            "        [ 1.0083e-02,  9.6251e-03, -4.4236e-02, -3.0073e-02,  8.6676e-02],\n",
            "        [ 7.7682e-02,  4.9190e-02, -1.2023e-01,  5.4921e-03, -3.9368e-02]],\n",
            "       grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.nhead = nhead\n",
        "        self.head_dim = d_model // nhead\n",
        "\n",
        "        self.query_linear = nn.Linear(d_model, d_model)\n",
        "        self.key_linear = nn.Linear(d_model, d_model)\n",
        "        self.value_linear = nn.Linear(d_model, d_model)\n",
        "        self.out_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "        return scores\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        query = self.query_linear(query)\n",
        "        key = self.key_linear(key)\n",
        "        value = self.value_linear(value)\n",
        "\n",
        "        query, key, value = [self.split_heads(x) for x in (query, key, value)]\n",
        "\n",
        "        scores = self.attention(query, key, value)\n",
        "        x = torch.matmul(scores, value)\n",
        "\n",
        "        x = self.combine_heads(x)\n",
        "        x = self.out_linear(x)\n",
        "\n",
        "        return self.dropout(x), torch.mean(scores, dim=-2)\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_len, features = x.size()\n",
        "        x = x.view(batch_size, seq_len, self.nhead, self.head_dim)\n",
        "        return x.permute(0, 2, 1, 3).contiguous().view(batch_size * self.nhead, seq_len, self.head_dim)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "        x = x.view(batch_size // self.nhead, self.nhead, seq_len, -1)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size // self.nhead, seq_len, -1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class OneDimCNN(nn.Module):\n",
        "    def __init__(self, max_byte_len, d_dim=256, kernel_size=[3, 4], filters=256, dropout=0.1):\n",
        "        super(OneDimCNN, self).__init__()\n",
        "        self.kernel_size = kernel_size\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv1d(in_channels=d_dim, out_channels=filters, kernel_size=h),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool1d(kernel_size=max_byte_len - h + 1)\n",
        "            ) for h in self.kernel_size\n",
        "        ])\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = [conv(x.transpose(-2, -1)) for conv in self.convs]\n",
        "        out = torch.cat(out, dim=1)\n",
        "        out = out.view(-1, out.size(1))\n",
        "        return self.dropout(out)\n",
        "\n",
        "\n",
        "class MHSAM(nn.Module):\n",
        "    def __init__(self, num_class, max_byte_len, kernel_size=[3, 4], d_dim=256, dropout=0.1, filters=256, nhead=4):\n",
        "        super(MHSAM, self).__init__()\n",
        "        self.posembedding = nn.Embedding(num_embeddings=max_byte_len, embedding_dim=d_dim)\n",
        "        self.byteembedding = nn.Embedding(num_embeddings=300, embedding_dim=d_dim)\n",
        "        self.attention = MultiHeadAttention(d_dim, nhead, dropout)\n",
        "        self.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "        self.fc = nn.Linear(in_features=filters * len(kernel_size), out_features=num_class)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.byteembedding(x) + self.posembedding(y)\n",
        "        out, _ = self.attention(out, out, out)\n",
        "        out = self.cnn(out)\n",
        "        out = self.fc(out)\n",
        "        if not self.training:\n",
        "            return F.softmax(out, dim=-1).max(1)[1]\n",
        "        return out\n",
        "\n",
        "# Exemple d'utilisation:\n",
        "x = torch.randint(0, 255, (10, 20))\n",
        "y = torch.randint(0, 20, (10, 20))\n",
        "mhsam = MHSAM(num_class=5, max_byte_len=20)\n",
        "output = mhsam(x, y)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainmhsam(i, flow_dict):\n",
        "    f = open('results_%d.txt' % i, 'w')\n",
        "    f.write('Train Loss Time Test\\n')\n",
        "    f.flush()\n",
        "\n",
        "    model = MHSAM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "    optimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "    loss_list = []\n",
        "\n",
        "    # default epoch is 3\n",
        "    for epoch_i in trange(3, mininterval=2, desc='  - (Training Epochs)   ', leave=False):\n",
        "        train_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "        training_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=train_x, y=train_y, label=train_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "        train_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "        test_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "        test_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=test_x, y=test_y, label=test_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=False\n",
        "        )\n",
        "        test_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "        with open('atten_%d.txt' % i, 'w') as f2:\n",
        "            f2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "        # write F1, PRECISION, RECALL\n",
        "        with open('metric_%d.txt' % i, 'w') as f3:\n",
        "            f3.write('F1 PRE REC\\n')\n",
        "            p, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "            for a, b, c in zip(fscore, p, r):\n",
        "                # for every cls\n",
        "                f3.write('%.2f %.2f %.2f\\n' % (a, b, c))\n",
        "                f3.flush()\n",
        "            if len(fscore) != len(protocols):\n",
        "                a = set(pred)\n",
        "                b = set(test_label[:, 0])\n",
        "                f3.write('%s\\n%s' % (str(a), str(b)))\n",
        "\n",
        "        # write Confusion Matrix\n",
        "        with open('cm_%d.pkl' % i, 'wb') as f4:\n",
        "            pickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "        # write ACC\n",
        "        f.write('%.2f %.4f %.6f %.2f\\n' % (train_acc, train_loss, test_time, test_acc))\n",
        "        f.flush()\n",
        "\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                               \r"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \tflow_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====\u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m fold validation ====\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmainmhsam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflow_dict\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[31], line 30\u001b[0m, in \u001b[0;36mmainmhsam\u001b[1;34m(i, flow_dict)\u001b[0m\n\u001b[0;32m     22\u001b[0m test_x, test_y, test_label \u001b[38;5;241m=\u001b[39m load_epoch_data(flow_dict, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m test_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m     24\u001b[0m     Dataset(x\u001b[38;5;241m=\u001b[39mtest_x, y\u001b[38;5;241m=\u001b[39mtest_y, label\u001b[38;5;241m=\u001b[39mtest_label),\n\u001b[0;32m     25\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     29\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m test_acc, score, pred, test_time \u001b[38;5;241m=\u001b[39m \u001b[43mtest_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124matten_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f2:\n\u001b[0;32m     32\u001b[0m     f2\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.4f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat, score)))\n",
            "Cell \u001b[1;32mIn[22], line 25\u001b[0m, in \u001b[0;36mtest_epoch\u001b[1;34m(model, test_data)\u001b[0m\n\u001b[0;32m     23\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m     24\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 25\u001b[0m pred, score \u001b[38;5;241m=\u001b[39m model(src_seq, src_seq2)\n\u001b[0;32m     26\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize()\n\u001b[0;32m     27\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
            "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainmhsam(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "kernel size, dropout and optimzer RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "class OneDimCNN(nn.Module):\n",
        "\t\"\"\"docstring for OneDimCNN\"\"\"\n",
        "\t# https://blog.csdn.net/sunny_xsc1994/article/details/82969867\n",
        "\tdef __init__(self, max_byte_len, d_dim=256, \\\n",
        "\t\tkernel_size = [3, 4], filters=256, dropout=0.09):\n",
        "\t\tsuper(OneDimCNN, self).__init__()\n",
        "\t\tself.kernel_size = kernel_size\n",
        "\t\tself.convs = nn.ModuleList([\n",
        "\t\t\t\t\t\tnn.Sequential(nn.Conv1d(in_channels=d_dim,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tout_channels=filters,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tkernel_size=h),\n",
        "\t\t\t\t\t\t#nn.BatchNorm1d(num_features=config.feature_size),\n",
        "\t\t\t\t\t\tnn.ReLU(),\n",
        "\t\t\t\t\t\t# MaxPool1d:\n",
        "\t\t\t\t\t\t# stride – the stride of the window. Default value is kernel_size\n",
        "\t\t\t\t\t\tnn.MaxPool1d(kernel_size=max_byte_len-h+1))\n",
        "\t\t\t\t\t\tfor h in self.kernel_size\n",
        "\t\t\t\t\t\t]\n",
        "\t\t\t\t\t\t)\n",
        "\t\tself.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tout = [conv(x.transpose(-2,-1)) for conv in self.convs]\n",
        "\t\tout = torch.cat(out, dim=1)\n",
        "\t\tout = out.view(-1, out.size(1))\n",
        "\t\treturn self.dropout(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SAMFiltre(nn.Module):\n",
        "    def __init__(self, num_class, max_byte_len, kernel_size=[3, 4], d_dim=256, dropout=0.09, filters=256):\n",
        "        super(SAMFiltre, self).__init__()\n",
        "        self.posembedding = nn.Embedding(num_embeddings=max_byte_len, embedding_dim=d_dim)\n",
        "        self.byteembedding = nn.Embedding(num_embeddings=300, embedding_dim=d_dim)\n",
        "        self.attention = SelfAttention(d_dim, dropout)\n",
        "        self.cnn = OneDimCNN(max_byte_len, d_dim, kernel_size, filters, dropout)\n",
        "        self.fc = nn.Linear(in_features=filters * len(kernel_size), out_features=num_class)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = self.byteembedding(x) + self.posembedding(y)\n",
        "        out, score = self.attention(out, out, out)\n",
        "        out = self.cnn(out)\n",
        "        out = self.fc(out)\n",
        "        if not self.training:\n",
        "            return F.softmax(out, dim=-1).max(1)[1], score\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainfiltre(i, flow_dict):\n",
        "    f = open('results_kernel%d.txt' % i, 'w')\n",
        "    f.write('Train Loss Time Test\\n')\n",
        "    f.flush()\n",
        "\n",
        "    model = SAMFiltre(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "    optimizer = optim.Adam(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "    loss_list = []\n",
        "\n",
        "    # default epoch is 3\n",
        "    for epoch_i in trange(3, mininterval=2, desc='  - (Training Epochs)   ', leave=False):\n",
        "        train_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "        training_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=train_x, y=train_y, label=train_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "        train_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "        test_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "        test_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=test_x, y=test_y, label=test_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=False\n",
        "        )\n",
        "        test_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "        with open('atten_%dkernel.txt' % i, 'w') as f2:\n",
        "            f2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "        # write F1, PRECISION, RECALL\n",
        "        with open('metric_%dkernel.txt' % i, 'w') as f3:\n",
        "            f3.write('F1 PRE REC\\n')\n",
        "            p, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "            for a, b, c in zip(fscore, p, r):\n",
        "                # for every cls\n",
        "                f3.write('%.2f %.2f %.2f\\n' % (a, b, c))\n",
        "                f3.flush()\n",
        "            if len(fscore) != len(protocols):\n",
        "                a = set(pred)\n",
        "                b = set(test_label[:, 0])\n",
        "                f3.write('%s\\n%s' % (str(a), str(b)))\n",
        "\n",
        "        # write Confusion Matrix\n",
        "        with open('cm_%dkernel.pkl' % i, 'wb') as f4:\n",
        "            pickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "        # write ACC\n",
        "        f.write('%.2f %.4f %.6f %.2f\\n' % (train_acc, train_loss, test_time, test_acc))\n",
        "        f.flush()\n",
        "\n",
        "    f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainfiltre(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "optimizer SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainSGD(i, flow_dict):\n",
        "    f = open('results_SGD%d.txt' % i, 'w')\n",
        "    f.write('Train Loss Time Test\\n')\n",
        "    f.flush()\n",
        "\n",
        "    model = SAM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "    optimizer = optim.SGD(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "    loss_list = []\n",
        "\n",
        "    # default epoch is 3\n",
        "    for epoch_i in trange(5, mininterval=2, desc='  - (Training Epochs)   ', leave=False):\n",
        "        train_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "        training_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=train_x, y=train_y, label=train_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "        train_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "        test_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "        test_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=test_x, y=test_y, label=test_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=False\n",
        "        )\n",
        "        test_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "        with open('atten_%dSGD.txt' % i, 'w') as f2:\n",
        "            f2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "        # write F1, PRECISION, RECALL\n",
        "        with open('metric_%dSGD.txt' % i, 'w') as f3:\n",
        "            f3.write('F1 PRE REC\\n')\n",
        "            p, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "            for a, b, c in zip(fscore, p, r):\n",
        "                # for every cls\n",
        "                f3.write('%.2f %.2f %.2f\\n' % (a, b, c))\n",
        "                f3.flush()\n",
        "            if len(fscore) != len(protocols):\n",
        "                a = set(pred)\n",
        "                b = set(test_label[:, 0])\n",
        "                f3.write('%s\\n%s' % (str(a), str(b)))\n",
        "\n",
        "        # write Confusion Matrix\n",
        "        with open('cm_%dSGD.pkl' % i, 'wb') as f4:\n",
        "            pickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "        # write ACC\n",
        "        f.write('%.2f %.4f %.6f %.2f\\n' % (train_acc, train_loss, test_time, test_acc))\n",
        "        f.flush()\n",
        "\n",
        "    f.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  - (Training Epochs)   :   0%|          | 0/5 [00:00<?, ?it/s]C:\\Users\\abayev\\AppData\\Local\\Temp\\ipykernel_14952\\2161614021.py:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
            "  return torch.LongTensor(x), torch.LongTensor(y), torch.LongTensor(label)\n",
            "c:\\Users\\abayev\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainSGD(i, flow_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "AdamW "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mainAdamW(i, flow_dict):\n",
        "    f = open('results_AdamW%d.txt' % i, 'w')\n",
        "    f.write('Train Loss Time Test\\n')\n",
        "    f.flush()\n",
        "\n",
        "    model = SAM(num_class=len(protocols), max_byte_len=max_byte_len).cuda()\n",
        "    optimizer = optim.AdamW(filter(lambda x: x.requires_grad, model.parameters()))\n",
        "    loss_list = []\n",
        "\n",
        "    # default epoch is 3\n",
        "    for epoch_i in trange(3, mininterval=2, desc='  - (Training Epochs)   ', leave=False):\n",
        "        train_x, train_y, train_label = load_epoch_data(flow_dict, 'train')\n",
        "        training_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=train_x, y=train_y, label=train_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=True\n",
        "        )\n",
        "        train_loss, train_acc = train_epoch(model, training_data, optimizer)\n",
        "\n",
        "        test_x, test_y, test_label = load_epoch_data(flow_dict, 'test')\n",
        "        test_data = torch.utils.data.DataLoader(\n",
        "            Dataset(x=test_x, y=test_y, label=test_label),\n",
        "            num_workers=0,\n",
        "            collate_fn=paired_collate_fn,\n",
        "            batch_size=128,\n",
        "            shuffle=False\n",
        "        )\n",
        "        test_acc, score, pred, test_time = test_epoch(model, test_data)\n",
        "        with open('atten_%dAdamW.txt' % i, 'w') as f2:\n",
        "            f2.write(' '.join(map('{:.4f}'.format, score)))\n",
        "\n",
        "        # write F1, PRECISION, RECALL\n",
        "        with open('metric_%dAdamW.txt' % i, 'w') as f3:\n",
        "            f3.write('F1 PRE REC\\n')\n",
        "            p, r, fscore, _ = precision_recall_fscore_support(test_label, pred)\n",
        "            for a, b, c in zip(fscore, p, r):\n",
        "                # for every cls\n",
        "                f3.write('%.2f %.2f %.2f\\n' % (a, b, c))\n",
        "                f3.flush()\n",
        "            if len(fscore) != len(protocols):\n",
        "                a = set(pred)\n",
        "                b = set(test_label[:, 0])\n",
        "                f3.write('%s\\n%s' % (str(a), str(b)))\n",
        "\n",
        "        # write Confusion Matrix\n",
        "        with open('cm_%dAdamW.pkl' % i, 'wb') as f4:\n",
        "            pickle.dump(confusion_matrix(test_label, pred, normalize='true'), f4)\n",
        "\n",
        "        # write ACC\n",
        "        f.write('%.2f %.4f %.6f %.2f\\n' % (train_acc, train_loss, test_time, test_acc))\n",
        "        f.flush()\n",
        "\n",
        "    f.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==== 0  fold validation ====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                        \r"
          ]
        }
      ],
      "source": [
        "for i in range(1):\n",
        "\t\twith open('pro_flows_%d_noip_fold.pkl'%i, 'rb') as f:\n",
        "\t\t\tflow_dict = pickle.load(f)\n",
        "\t\tprint('====', i, ' fold validation ====')\n",
        "\t\tmainAdamW(i, flow_dict)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
